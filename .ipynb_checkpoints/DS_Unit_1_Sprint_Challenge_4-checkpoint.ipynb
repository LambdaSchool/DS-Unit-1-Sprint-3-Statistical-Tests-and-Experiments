{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NooAiTdnafkz"
   },
   "source": [
    "# Data Science Unit 1 Sprint Challenge 4\n",
    "\n",
    "## Exploring Data, Testing Hypotheses\n",
    "\n",
    "In this sprint challenge you will look at a dataset of people being approved or rejected for credit.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Credit+Approval\n",
    "\n",
    "Data Set Information: This file concerns credit card applications. All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data. This dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values. There are also a few missing values.\n",
    "\n",
    "Attribute Information:\n",
    "- A1: b, a.\n",
    "- A2: continuous.\n",
    "- A3: continuous.\n",
    "- A4: u, y, l, t.\n",
    "- A5: g, p, gg.\n",
    "- A6: c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff.\n",
    "- A7: v, h, bb, j, n, z, dd, ff, o.\n",
    "- A8: continuous.\n",
    "- A9: t, f.\n",
    "- A10: t, f.\n",
    "- A11: continuous.\n",
    "- A12: t, f.\n",
    "- A13: g, p, s.\n",
    "- A14: continuous.\n",
    "- A15: continuous.\n",
    "- A16: +,- (class attribute)\n",
    "\n",
    "Yes, most of that doesn't mean anything. A16 (the class attribute) is the most interesting, as it separates the 307 approved cases from the 383 rejected cases. The remaining variables have been obfuscated for privacy - a challenge you may have to deal with in your data science career.\n",
    "\n",
    "Sprint challenges are evaluated based on satisfactory completion of each part. It is suggested you work through it in order, getting each aspect reasonably working, before trying to deeply explore, iterate, or refine any given step. Once you get to the end, if you want to go back and improve things, go for it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5wch6ksCbJtZ"
   },
   "source": [
    "## Part 1 - Load and validate the data\n",
    "\n",
    "- Load the data as a `pandas` data frame.\n",
    "- Validate that it has the appropriate number of observations (you can check the raw file, and also read the dataset description from UCI).\n",
    "- UCI says there should be missing data - check, and if necessary change the data so pandas recognizes it as na\n",
    "- Make sure that the loaded features are of the types described above (continuous values should be treated as float), and correct as necessary\n",
    "\n",
    "This is review, but skills that you'll use at the start of any data exploration. Further, you may have to do some investigation to figure out which file to load from - that is part of the puzzle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q79xDLckzibS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data' \n",
    "names = ['A1','A2','A3','A4','A5','A6','A7','A8','A9','A10','A11','A12','A13','A14','A15','approval']\n",
    "data = pd.read_csv(url, na_values='?', names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(690, 16)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.replace(['+','-'],[True,False], inplace=True)\n",
    "data.replace(['t','f'],[True,False], inplace=True)\n",
    "\n",
    "# Validate that it has the appropriate number of observations\n",
    "data.shape # Expected observations: 690"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A1          12\n",
       "A2          12\n",
       "A3           0\n",
       "A4           6\n",
       "A5           6\n",
       "A6           9\n",
       "A7           9\n",
       "A8           0\n",
       "A9           0\n",
       "A10          0\n",
       "A11          0\n",
       "A12          0\n",
       "A13          0\n",
       "A14         13\n",
       "A15          0\n",
       "approval     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing data, compare to source. Already changed \"?\" to NA.\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[str,\n",
       " numpy.float64,\n",
       " numpy.float64,\n",
       " str,\n",
       " str,\n",
       " str,\n",
       " str,\n",
       " numpy.float64,\n",
       " numpy.bool_,\n",
       " numpy.bool_,\n",
       " numpy.int64,\n",
       " numpy.bool_,\n",
       " str,\n",
       " numpy.float64,\n",
       " numpy.int64,\n",
       " numpy.bool_]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure that the loaded features are of the types described above \n",
    "# (continuous values should be treated as float), \n",
    "# and correct as necessary\n",
    "\n",
    "[type(data[col][0]) for col in data.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[str,\n",
       " numpy.float64,\n",
       " numpy.float64,\n",
       " str,\n",
       " str,\n",
       " str,\n",
       " str,\n",
       " numpy.float64,\n",
       " numpy.bool_,\n",
       " numpy.bool_,\n",
       " numpy.float64,\n",
       " numpy.bool_,\n",
       " str,\n",
       " numpy.float64,\n",
       " numpy.float64,\n",
       " numpy.bool_]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A11 and A15 are int64; let's turn them into float64\n",
    "data2 = data.astype({'A11':np.float64, 'A15':np.float64})\n",
    "\n",
    "[type(data2[col][0]) for col in data2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data2 looks good, so I'll backpropagate\n",
    "data = data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7rLytbrO38L"
   },
   "source": [
    "## Part 2 - Exploring data, Testing hypotheses\n",
    "\n",
    "The only thing we really know about this data is that A16 is the class label. Besides that, we have 6 continuous (float) features and 9 categorical features.\n",
    "\n",
    "Explore the data: you can use whatever approach (tables, utility functions, visualizations) to get an impression of the distributions and relationships of the variables. In general, your goal is to understand how the features are different when grouped by the two class labels (`+` and `-`).\n",
    "\n",
    "For the 6 continuous features, how are they different when split between the two class labels? Choose two features to run t-tests (again split by class label) - specifically, select one feature that is *extremely* different between the classes, and another feature that is notably less different (though perhaps still \"statistically significantly\" different). You may have to explore more than two features to do this.\n",
    "\n",
    "For the categorical features, explore by creating \"cross tabs\" between them and the class label, and apply the Chi-squared test to them. There are 9 categorical features - as with the t-test, try to find one where the Chi-squared test returns an extreme result (rejecting the null that the data are independent), and one where it is less extreme.\n",
    "\n",
    "**NOTE** - \"less extreme\" just means smaller test statistic/larger p-value. Even the least extreme differences may be strongly statistically significant.\n",
    "\n",
    "Your *main* goal is the hypothesis tests, so don't spend too much time on the exploration/visualization piece. That is just a means to an end. This is challenging, so manage your time and aim for a baseline of at least running two t-tests and two Chi-squared tests before polishing. And don't forget to answer the questions in part 3, even if your results in this part aren't what you want them to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_nqcgc0yzm68"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A8</th>\n",
       "      <th>A11</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>29.808231</td>\n",
       "      <td>3.839948</td>\n",
       "      <td>1.257924</td>\n",
       "      <td>0.631854</td>\n",
       "      <td>199.699468</td>\n",
       "      <td>198.605744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>33.720492</td>\n",
       "      <td>5.904951</td>\n",
       "      <td>3.427899</td>\n",
       "      <td>4.605863</td>\n",
       "      <td>164.421927</td>\n",
       "      <td>2038.859935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 A2        A3        A8       A11         A14          A15\n",
       "approval                                                                  \n",
       "False     29.808231  3.839948  1.257924  0.631854  199.699468   198.605744\n",
       "True      33.720492  5.904951  3.427899  4.605863  164.421927  2038.859935"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuous_columns = ['A2','A3','A8','A11','A14','A15']\n",
    "\n",
    "cont_groups = data.groupby('approval')[continuous_columns]\n",
    "\n",
    "# How do the mean values for each column compare between the classes?\n",
    "cont_groups.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A8</th>\n",
       "      <th>A11</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>10.919291</td>\n",
       "      <td>4.337662</td>\n",
       "      <td>2.120481</td>\n",
       "      <td>1.900049</td>\n",
       "      <td>181.564835</td>\n",
       "      <td>671.608839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>12.809641</td>\n",
       "      <td>5.471485</td>\n",
       "      <td>4.120792</td>\n",
       "      <td>6.320242</td>\n",
       "      <td>161.770675</td>\n",
       "      <td>7659.763941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 A2        A3        A8       A11         A14          A15\n",
       "approval                                                                  \n",
       "False     10.919291  4.337662  2.120481  1.900049  181.564835   671.608839\n",
       "True      12.809641  5.471485  4.120792  6.320242  161.770675  7659.763941"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_groups.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2  >>   t-Stat:  4.29,   P-val: 2.028e-05\n",
      "A3  >>   t-Stat:  5.53,   P-val: 4.552e-08\n",
      "A8  >>   t-Stat:  8.94,   P-val: 3.671e-18\n",
      "A11 >>   t-Stat:  11.7,   P-val: 7.958e-29\n",
      "A14 >>   t-Stat: -2.64,   P-val: 0.008586\n",
      "A15 >>   t-Stat:  4.68,   P-val: 3.452e-06\n"
     ]
    }
   ],
   "source": [
    "# Here I test the difference between both values of approval, \n",
    "# for all of the continuous features. Better than continuing to look\n",
    "# at descriptive statistics like the above.\n",
    "\n",
    "\n",
    "# Iterate through the continuous columns\n",
    "for col in continuous_columns:\n",
    "    # Create a dataframe with 2 columns: the continuous one and 'approval',\n",
    "#     then group it by approval\n",
    "    grouped_column = data[[col,'approval']].groupby('approval')\n",
    "    \n",
    "    # Create an array with just the data for each value of 'approval'\n",
    "    yes_approved = grouped_column.get_group(True)[col]\n",
    "    not_approved = grouped_column.get_group(False)[col]\n",
    "    \n",
    "    # Figure out whether they're statistically different\n",
    "    t_stat, p_val = stats.ttest_ind(yes_approved,\n",
    "                                    not_approved, \n",
    "                                    nan_policy='omit')\n",
    "    print('{:3} >>   t-Stat: {:5.3},   P-val: {:.4}'.format(col, \n",
    "                                                          t_stat, \n",
    "                                                          p_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1  >>   Chi2-Stat:      0.311,   P-val: 0.5769\n",
      "A4  >>   Chi2-Stat:       26.2,   P-val: 2.011e-06\n",
      "A5  >>   Chi2-Stat:       26.2,   P-val: 2.011e-06\n",
      "A6  >>   Chi2-Stat:       98.3,   P-val: 3.5e-15\n",
      "A7  >>   Chi2-Stat:       45.0,   P-val: 3.625e-07\n",
      "A9  >>   Chi2-Stat:   3.55e+02,   P-val: 3.119e-79\n",
      "A10 >>   Chi2-Stat:   1.43e+02,   P-val: 5.676e-33\n",
      "A12 >>   Chi2-Stat:      0.568,   P-val: 0.4509\n",
      "A13 >>   Chi2-Stat:       9.19,   P-val: 0.01009\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['A1','A4','A5','A6','A7','A9','A10','A12','A13']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    local_set = pd.crosstab(data[col], data['approval'])\n",
    "    chi2_stat, p_val, dof, ex = stats.chi2_contingency(local_set)\n",
    "\n",
    "    print('{:3} >>   Chi2-Stat: {:10.3},   P-val: {:.4}'.format(col, \n",
    "                                                          chi2_stat, \n",
    "                                                          p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZM8JckA2bgnp"
   },
   "source": [
    "## Part 3 - Analysis and Interpretation\n",
    "\n",
    "Now that you've looked at the data, answer the following questions:\n",
    "\n",
    "- Interpret and explain the two t-tests you ran - what do they tell you about the relationships between the continuous features you selected and the class labels?\n",
    "- Interpret and explain the two Chi-squared tests you ran - what do they tell you about the relationships between the categorical features you selected and the class labels?\n",
    "- What was the most challenging part of this sprint challenge?\n",
    "\n",
    "Answer with text, but feel free to intersperse example code/results or refer to it from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LIozLDNG2Uhu"
   },
   "source": [
    "### Interpret and explain the two t-tests you ran \n",
    "I ran the t-test for every continuous feature in the dataset, comparing the class labels against each other. The t-statistic was really significant for all of them.  The least significant was A14, with P = 0.0086 (still above 99% confidence).\n",
    "\n",
    "Each of these significant results tells us that the two groups (approved and not-approved) had different mean values of that feature.  Our null hypothesis is that the values of a feature are being sampled from the same distribution for both values of the class label (i.e., that the two groups were indistinguishable by that feature). The P-value is the probability that we would get values at least as extreme as we observed just by chance, if the null hypothesis were true.  Because those P-values are all very small (except maye 0.0086), we reject hte null hypothesis and say that the two groups (approved and not approved) were drawn from different distributions.  That is, the people that were and weren't approved showed significantly different values of that one continuous feature.\n",
    "\n",
    "### Interpret and explain the two Chi-squared tests you ran\n",
    "I also ran the Chi2 test on all the features (specifically, a Chi-square test of independence of variables in a contingency table). For each feature, I created a crosstable of that feature against the class feature ('approval).\n",
    "\n",
    "chi2_contingency is a test of the null hypothesis that the differences between the frequencies observed in the contingency table arose by chance.  You'd expect the null hypothesis if the frequencies observed for the values of that feature did **not** affect the loan decision.  Whenever the test yielded a really low p-value, we reject the null hypothesis and conclude that that feature **did** have an effect on approval.\n",
    "\n",
    "Looking at the results, it seems that most categorical variables strongly affected the decision.  The exceptions are A1 (P-val: 0.5769), A12 (P-val: 0.4509), and arguably A13 (P-val: 0.01). \n",
    "\n",
    "### What was the most challenging part of this sprint challenge?\n",
    "Groupby and crosstab.  Again.  In retrospect, I didn't even need to use groupby at all.  For these few values of the class feature, I should've manually divided the dataset into approved and rejected.  I guess I know what to go back and study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS Unit 1 Sprint Challenge 4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
