{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJGtmni-DezY"
   },
   "source": [
    "# Lambda School Data Science Module 141\n",
    "## Statistics, Probability, and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FMhDKOFND0qY"
   },
   "source": [
    "## Prepare - examine what's available in SciPy\n",
    "\n",
    "As we delve into statistics, we'll be using more libraries - in particular the [stats package from SciPy](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4427
    },
    "colab_type": "code",
    "id": "fQ9rkLJmEbsk",
    "outputId": "937d6c40-d775-4016-9b69-70a82cc8b4c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_binned_statistic',\n",
       " '_constants',\n",
       " '_continuous_distns',\n",
       " '_discrete_distns',\n",
       " '_distn_infrastructure',\n",
       " '_distr_params',\n",
       " '_multivariate',\n",
       " '_stats',\n",
       " '_stats_mstats_common',\n",
       " '_tukeylambda_stats',\n",
       " 'absolute_import',\n",
       " 'alpha',\n",
       " 'anderson',\n",
       " 'anderson_ksamp',\n",
       " 'anglit',\n",
       " 'ansari',\n",
       " 'arcsine',\n",
       " 'argus',\n",
       " 'bartlett',\n",
       " 'bayes_mvs',\n",
       " 'bernoulli',\n",
       " 'beta',\n",
       " 'betaprime',\n",
       " 'binned_statistic',\n",
       " 'binned_statistic_2d',\n",
       " 'binned_statistic_dd',\n",
       " 'binom',\n",
       " 'binom_test',\n",
       " 'boltzmann',\n",
       " 'boxcox',\n",
       " 'boxcox_llf',\n",
       " 'boxcox_normmax',\n",
       " 'boxcox_normplot',\n",
       " 'bradford',\n",
       " 'burr',\n",
       " 'burr12',\n",
       " 'cauchy',\n",
       " 'chi',\n",
       " 'chi2',\n",
       " 'chi2_contingency',\n",
       " 'chisquare',\n",
       " 'circmean',\n",
       " 'circstd',\n",
       " 'circvar',\n",
       " 'combine_pvalues',\n",
       " 'contingency',\n",
       " 'cosine',\n",
       " 'crystalball',\n",
       " 'cumfreq',\n",
       " 'describe',\n",
       " 'dgamma',\n",
       " 'dirichlet',\n",
       " 'distributions',\n",
       " 'division',\n",
       " 'dlaplace',\n",
       " 'dweibull',\n",
       " 'energy_distance',\n",
       " 'entropy',\n",
       " 'erlang',\n",
       " 'expon',\n",
       " 'exponnorm',\n",
       " 'exponpow',\n",
       " 'exponweib',\n",
       " 'f',\n",
       " 'f_oneway',\n",
       " 'fatiguelife',\n",
       " 'find_repeats',\n",
       " 'fisher_exact',\n",
       " 'fisk',\n",
       " 'fligner',\n",
       " 'foldcauchy',\n",
       " 'foldnorm',\n",
       " 'frechet_l',\n",
       " 'frechet_r',\n",
       " 'friedmanchisquare',\n",
       " 'gamma',\n",
       " 'gausshyper',\n",
       " 'gaussian_kde',\n",
       " 'genexpon',\n",
       " 'genextreme',\n",
       " 'gengamma',\n",
       " 'genhalflogistic',\n",
       " 'genlogistic',\n",
       " 'gennorm',\n",
       " 'genpareto',\n",
       " 'geom',\n",
       " 'gilbrat',\n",
       " 'gmean',\n",
       " 'gompertz',\n",
       " 'gumbel_l',\n",
       " 'gumbel_r',\n",
       " 'halfcauchy',\n",
       " 'halfgennorm',\n",
       " 'halflogistic',\n",
       " 'halfnorm',\n",
       " 'hmean',\n",
       " 'hypergeom',\n",
       " 'hypsecant',\n",
       " 'invgamma',\n",
       " 'invgauss',\n",
       " 'invweibull',\n",
       " 'invwishart',\n",
       " 'iqr',\n",
       " 'itemfreq',\n",
       " 'jarque_bera',\n",
       " 'johnsonsb',\n",
       " 'johnsonsu',\n",
       " 'kappa3',\n",
       " 'kappa4',\n",
       " 'kde',\n",
       " 'kendalltau',\n",
       " 'kruskal',\n",
       " 'ks_2samp',\n",
       " 'ksone',\n",
       " 'kstat',\n",
       " 'kstatvar',\n",
       " 'kstest',\n",
       " 'kstwobign',\n",
       " 'kurtosis',\n",
       " 'kurtosistest',\n",
       " 'laplace',\n",
       " 'levene',\n",
       " 'levy',\n",
       " 'levy_l',\n",
       " 'levy_stable',\n",
       " 'linregress',\n",
       " 'loggamma',\n",
       " 'logistic',\n",
       " 'loglaplace',\n",
       " 'lognorm',\n",
       " 'logser',\n",
       " 'lomax',\n",
       " 'mannwhitneyu',\n",
       " 'matrix_normal',\n",
       " 'maxwell',\n",
       " 'median_test',\n",
       " 'mielke',\n",
       " 'mode',\n",
       " 'moment',\n",
       " 'mood',\n",
       " 'morestats',\n",
       " 'moyal',\n",
       " 'mstats',\n",
       " 'mstats_basic',\n",
       " 'mstats_extras',\n",
       " 'multinomial',\n",
       " 'multivariate_normal',\n",
       " 'mvn',\n",
       " 'mvsdist',\n",
       " 'nakagami',\n",
       " 'nbinom',\n",
       " 'ncf',\n",
       " 'nct',\n",
       " 'ncx2',\n",
       " 'norm',\n",
       " 'normaltest',\n",
       " 'norminvgauss',\n",
       " 'obrientransform',\n",
       " 'ortho_group',\n",
       " 'pareto',\n",
       " 'pearson3',\n",
       " 'pearsonr',\n",
       " 'percentileofscore',\n",
       " 'planck',\n",
       " 'pointbiserialr',\n",
       " 'poisson',\n",
       " 'power_divergence',\n",
       " 'powerlaw',\n",
       " 'powerlognorm',\n",
       " 'powernorm',\n",
       " 'ppcc_max',\n",
       " 'ppcc_plot',\n",
       " 'print_function',\n",
       " 'probplot',\n",
       " 'randint',\n",
       " 'random_correlation',\n",
       " 'rankdata',\n",
       " 'ranksums',\n",
       " 'rayleigh',\n",
       " 'rdist',\n",
       " 'recipinvgauss',\n",
       " 'reciprocal',\n",
       " 'relfreq',\n",
       " 'rice',\n",
       " 'rv_continuous',\n",
       " 'rv_discrete',\n",
       " 'rv_histogram',\n",
       " 'scoreatpercentile',\n",
       " 'sem',\n",
       " 'semicircular',\n",
       " 'shapiro',\n",
       " 'sigmaclip',\n",
       " 'skellam',\n",
       " 'skew',\n",
       " 'skewnorm',\n",
       " 'skewtest',\n",
       " 'spearmanr',\n",
       " 'special_ortho_group',\n",
       " 'statlib',\n",
       " 'stats',\n",
       " 't',\n",
       " 'test',\n",
       " 'theilslopes',\n",
       " 'tiecorrect',\n",
       " 'tmax',\n",
       " 'tmean',\n",
       " 'tmin',\n",
       " 'trapz',\n",
       " 'triang',\n",
       " 'trim1',\n",
       " 'trim_mean',\n",
       " 'trimboth',\n",
       " 'truncexpon',\n",
       " 'truncnorm',\n",
       " 'tsem',\n",
       " 'tstd',\n",
       " 'ttest_1samp',\n",
       " 'ttest_ind',\n",
       " 'ttest_ind_from_stats',\n",
       " 'ttest_rel',\n",
       " 'tukeylambda',\n",
       " 'tvar',\n",
       " 'uniform',\n",
       " 'unitary_group',\n",
       " 'variation',\n",
       " 'vonmises',\n",
       " 'vonmises_line',\n",
       " 'wald',\n",
       " 'wasserstein_distance',\n",
       " 'weibull_max',\n",
       " 'weibull_min',\n",
       " 'weightedtau',\n",
       " 'wilcoxon',\n",
       " 'wishart',\n",
       " 'wrapcauchy',\n",
       " 'zipf',\n",
       " 'zmap',\n",
       " 'zscore']"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "dir(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "bxW4SG_gJGlZ",
    "outputId": "e715ad1a-883f-41e2-b070-a1106316f4e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# As usual, lots of stuff here! There's our friend, the normal distribution\n",
    "norm = stats.norm()\n",
    "print(norm.mean())\n",
    "print(norm.std())\n",
    "print(norm.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "RyNKPt_tJk86",
    "outputId": "db64f558-1945-4fef-f7d7-3184212d8237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.2909944487358056\n",
      "1.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "# And a new friend - t\n",
    "t1 = stats.t(5)  # 5 is df \"shape\" parameter\n",
    "print(t1.mean())\n",
    "print(t1.std())\n",
    "print(t1.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRn1zMuaKgxX"
   },
   "source": [
    "![T distribution PDF with different shape parameters](https://upload.wikimedia.org/wikipedia/commons/4/41/Student_t_pdf.svg)\n",
    "\n",
    "*(Picture from [Wikipedia](https://en.wikipedia.org/wiki/Student's_t-distribution#/media/File:Student_t_pdf.svg))*\n",
    "\n",
    "The t-distribution is \"normal-ish\" - the larger the parameter (which reflects its degrees of freedom - more input data/features will increase it), the closer to true normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "seQv5unnJvpM",
    "outputId": "b2f84397-b204-4864-84a1-2b29eb926bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0350983390135313\n",
      "1.0714285714285714\n"
     ]
    }
   ],
   "source": [
    "t2 = stats.t(30)  # Will be closer to normal\n",
    "print(t2.mean())\n",
    "print(t2.std())\n",
    "print(t2.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FOvEGMysLaE2"
   },
   "source": [
    "Why is it different from normal? To better reflect the tendencies of small data and situations with unknown population standard deviation. In other words, the normal distribution is still the nice pure ideal in the limit (thanks to the central limit theorem), but the t-distribution is much more useful in many real-world situations.\n",
    "\n",
    "History sidenote - this is \"Student\":\n",
    "\n",
    "![William Sealy Gosset](https://upload.wikimedia.org/wikipedia/commons/4/42/William_Sealy_Gosset.jpg)\n",
    "\n",
    "*(Picture from [Wikipedia](https://en.wikipedia.org/wiki/File:William_Sealy_Gosset.jpg))*\n",
    "\n",
    "His real name is William Sealy Gosset, and he published under the pen name \"Student\" because he was not an academic. He was a brewer, working at Guinness and using trial and error to determine the best ways to yield barley. He's also proof that, even 100 years ago, you don't need official credentials to do real data science!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1yx_QilAEC6o"
   },
   "source": [
    "## Live Lecture - let's perform and interpret a t-test\n",
    "\n",
    "We'll generate our own data, so we can know and alter the \"ground truth\" that the t-test should find. We will learn about p-values and how to interpret \"statistical significance\" based on the output of a hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BuysRPs-Ed0v"
   },
   "outputs": [],
   "source": [
    "# TODO - during class, but please help!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "egXb7YpqEcZF"
   },
   "source": [
    "## Assignment - apply the t-test to real data\n",
    "\n",
    "Your assignment is to determine which issues have \"statistically significant\" differences between political parties in this [1980s congressional voting data](https://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records). The data consists of 435 instances (one for each congressperson), a class (democrat or republican), and 16 binary attributes (yes or no for voting for or against certain issues). Be aware - there are missing values!\n",
    "\n",
    "Your goals:\n",
    "\n",
    "1. Load and clean the data (or determine the best method to drop observations when running tests)\n",
    "2. Using hypothesis testing, find an issue that democrats support more than republicans with p < 0.01\n",
    "3. Using hypothesis testing, find an issue that republicans support more than democrats with p < 0.01\n",
    "4. Using hypothesis testing, find an issue where the difference between republicans and democrats has p > 0.1 (i.e. there may not be much of a difference)\n",
    "\n",
    "Note that this data will involve *2 sample* t-tests, because you're comparing averages across two groups (republicans and democrats) rather than a single group against a null hypothesis.\n",
    "\n",
    "Stretch goals:\n",
    "\n",
    "1. Refactor your code into functions so it's easy to rerun with arbitrary variables\n",
    "2. Apply hypothesis testing to your personal project data (for the purposes of this notebook you can type a summary of the hypothesis you formed and tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nstrmCG-Ecyk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data' \n",
    "names = ['party','handicapped-infants','water-project-cost-sharing','adoption-of-the-budget-resolution','physician-fee-freeze','el-salvador-aid','religious-groups-in-schools','anti-satellite-test-ban','aid-to-nicaraguan-contras','mx-missile','immigration','synfuels-corporation-cutback','education-spending','superfund-right-to-sue','crime','duty-free-exports','export-administration-act-south-africa']\n",
    "issues = names[1:]\n",
    "roster = pd.read_csv(url, na_values='?', names=names)\n",
    "roster.replace(['y','n'],[1,0], inplace=True)\n",
    "\n",
    "reps = roster[roster.party == 'republican']\n",
    "dems = roster[roster.party == 'democrat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue                                    Statistic  p_value\n",
      "------------------------------------------------------------\n",
      "handicapped-infants                      9.66       5.43e-20\n",
      "water-project-cost-sharing               -0.0889    0.929\n",
      "adoption-of-the-budget-resolution        22.8       1.95e-69\n",
      "physician-fee-freeze                     -56.7      8.82e-193\n",
      "el-salvador-aid                          -23.9      2.9e-79\n",
      "religious-groups-in-schools              -10.8      4.83e-24\n",
      "anti-satellite-test-ban                  12.5       1.36e-29\n",
      "aid-to-nicaraguan-contras                18.3       2.29e-52\n",
      "mx-missile                               17.4       4.29e-51\n",
      "immigration                              -1.74      0.0832\n",
      "synfuels-corporation-cutback             9.04       6.3e-18\n",
      "education-spending                       -20.7      3.57e-62\n",
      "superfund-right-to-sue                   -14.4      7.6e-38\n",
      "crime                                    -19.9      6.86e-58\n",
      "duty-free-exports                        14.4       3.81e-38\n",
      "export-administration-act-south-africa   6.4        1.04e-09\n"
     ]
    }
   ],
   "source": [
    "dem_favs = []\n",
    "rep_favs = []\n",
    "bipartisan = []\n",
    "other = []\n",
    "\n",
    "print('{:40} {:<10} {}'.format('Issue', 'Statistic', 'p_value'))\n",
    "print('-'*60)\n",
    "\n",
    "for issue in issues:    \n",
    "    stat, p_value = stats.ttest_ind(dems[issue], reps[issue],\n",
    "                                    nan_policy='omit', equal_var=False)\n",
    "    print('{:40} {:<10.3} {:.3}'.format(issue, stat, p_value))\n",
    "\n",
    "    if p_value < 0.01 and stat > 0:\n",
    "        dem_favs.append(issue)\n",
    "    elif p_value < 0.01 and stat < 0:\n",
    "        rep_favs.append(issue)\n",
    "    elif p_value > 0.1:\n",
    "        bipartisan.append(issue)\n",
    "    else:\n",
    "        other.append(issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['handicapped-infants',\n",
       " 'adoption-of-the-budget-resolution',\n",
       " 'anti-satellite-test-ban',\n",
       " 'aid-to-nicaraguan-contras',\n",
       " 'mx-missile',\n",
       " 'synfuels-corporation-cutback',\n",
       " 'duty-free-exports',\n",
       " 'export-administration-act-south-africa']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Issues favored by democrats, p < 0.01\n",
    "dem_favs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['physician-fee-freeze',\n",
       " 'el-salvador-aid',\n",
       " 'religious-groups-in-schools',\n",
       " 'education-spending',\n",
       " 'superfund-right-to-sue',\n",
       " 'crime']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Issues favored by republicans, p < 0.01\n",
    "rep_favs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['water-project-cost-sharing']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Issues favored by neither (P > 0.1)\n",
    "bipartisan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['immigration']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch goal!  Refactoring the code above into functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partisan_printout(roster, significance_cutoff=0.01,\n",
    "                     bipartisan_cutoff=0.1):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe of congressional voting and identifies the\n",
    "    issues on which there was a significant voting difference between\n",
    "    Republicans and Democrats. Comparison is made using Welch's t-test,\n",
    "    which does not assume equal variances or sample sizes.\n",
    "    \n",
    "    Input parameters:\n",
    "    \n",
    "    roster\n",
    "        A dataframe in which the column 'party' contains\n",
    "        classifies the congressperson as 'republican' or 'democrat',\n",
    "        the other columns are names of issues,\n",
    "        and each cell contains 0 (no vote), 1 (yes vote), or NaN.\n",
    "\n",
    "    significance_cutoff\n",
    "        Maximum p-value (non-inclusive) for an issue to be considered\n",
    "        significantly partisan\n",
    "        \n",
    "    bipartisan_cutoff\n",
    "        Minimum p-value (non-inclusive) for an issue to be considered\n",
    "        bipartisan\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    Printout with all the issues, classified by partisan preference.\n",
    "    \"\"\"\n",
    "    reps = roster[roster.party == 'republican']\n",
    "    dems = roster[roster.party == 'democrat']\n",
    "    issues = roster.columns[1:]\n",
    "\n",
    "    dem_favs = []\n",
    "    rep_favs = []\n",
    "    bipartisan = []\n",
    "    other = []\n",
    "    \n",
    "#   I populate the lists above with the issues that fall in each category. Each issue is a tuple of \n",
    "#   name, state, and p_value\n",
    "    for issue in issues:    \n",
    "        # Calculates Welch's t-test for each issue, comparing Dems to Reps.\n",
    "        stat, p_value = stats.ttest_ind(dems[issue], reps[issue],\n",
    "                                        nan_policy='omit', \n",
    "                                        equal_var=False)\n",
    "\n",
    "        \n",
    "        if p_value < significance_cutoff and stat > 0:\n",
    "            dem_favs.append((issue, stat, p_value))\n",
    "            \n",
    "        elif p_value < significance_cutoff and stat < 0:\n",
    "            rep_favs.append((issue, stat, p_value))\n",
    "            \n",
    "        elif p_value > bipartisan_cutoff:\n",
    "            bipartisan.append((issue, stat, p_value))\n",
    "            \n",
    "        else:\n",
    "            other.append((issue, stat, p_value))\n",
    "            \n",
    "            \n",
    "    # This block is for printing out the contents of all the categories above\n",
    "    classifications = [dem_favs, rep_favs, bipartisan, other]\n",
    "    headers = ['Issues favored by Democrats, p<{}',\n",
    "                'Issues favored by Republicans, p<{}',\n",
    "                'Bipartisan Issues, p>{}',\n",
    "                'Other Issues']\n",
    "    cutoffs = [significance_cutoff, significance_cutoff, bipartisan_cutoff, 0]\n",
    "    \n",
    "    for header, classification, cutoff in zip(headers, classifications, cutoffs):\n",
    "        print('{:50} {:<20} {}'.format('\\n\\n'+header.format(cutoff), \"Welch's t-test\", 'p_value'))\n",
    "        print('-'*80)\n",
    "    \n",
    "        for issue in classification:\n",
    "            print('{:50} {:<18.3} {:.3}'.format(issue[0], issue[1], issue[2]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Issues favored by Democrats, p<0.01              Welch's t-test       p_value\n",
      "--------------------------------------------------------------------------------\n",
      "handicapped-infants                                9.66               5.43e-20\n",
      "adoption-of-the-budget-resolution                  22.8               1.95e-69\n",
      "anti-satellite-test-ban                            12.5               1.36e-29\n",
      "aid-to-nicaraguan-contras                          18.3               2.29e-52\n",
      "mx-missile                                         17.4               4.29e-51\n",
      "synfuels-corporation-cutback                       9.04               6.3e-18\n",
      "duty-free-exports                                  14.4               3.81e-38\n",
      "export-administration-act-south-africa             6.4                1.04e-09\n",
      "\n",
      "\n",
      "Issues favored by Republicans, p<0.01            Welch's t-test       p_value\n",
      "--------------------------------------------------------------------------------\n",
      "physician-fee-freeze                               -56.7              8.82e-193\n",
      "el-salvador-aid                                    -23.9              2.9e-79\n",
      "religious-groups-in-schools                        -10.8              4.83e-24\n",
      "education-spending                                 -20.7              3.57e-62\n",
      "superfund-right-to-sue                             -14.4              7.6e-38\n",
      "crime                                              -19.9              6.86e-58\n",
      "\n",
      "\n",
      "Bipartisan Issues, p>0.1                         Welch's t-test       p_value\n",
      "--------------------------------------------------------------------------------\n",
      "water-project-cost-sharing                         -0.0889            0.929\n",
      "\n",
      "\n",
      "Other Issues                                     Welch's t-test       p_value\n",
      "--------------------------------------------------------------------------------\n",
      "immigration                                        -1.74              0.0832\n"
     ]
    }
   ],
   "source": [
    "partisan_printout(roster, 0.01, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS DS 141 Statistics Probability and Inference.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
