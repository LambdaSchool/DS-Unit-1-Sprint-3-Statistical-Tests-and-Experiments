{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS_Unit_1_Sprint_Challenge_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brit228/DS-Unit-1-Sprint-4-Statistical-Tests-and-Experiments/blob/master/DS_Unit_1_Sprint_Challenge_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NooAiTdnafkz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Science Unit 1 Sprint Challenge 4\n",
        "\n",
        "## Exploring Data, Testing Hypotheses\n",
        "\n",
        "In this sprint challenge you will look at a dataset of people being approved or rejected for credit.\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Credit+Approval\n",
        "\n",
        "Data Set Information: This file concerns credit card applications. All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data. This dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values. There are also a few missing values.\n",
        "\n",
        "Attribute Information:\n",
        "- A1: b, a.\n",
        "- A2: continuous.\n",
        "- A3: continuous.\n",
        "- A4: u, y, l, t.\n",
        "- A5: g, p, gg.\n",
        "- A6: c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff.\n",
        "- A7: v, h, bb, j, n, z, dd, ff, o.\n",
        "- A8: continuous.\n",
        "- A9: t, f.\n",
        "- A10: t, f.\n",
        "- A11: continuous.\n",
        "- A12: t, f.\n",
        "- A13: g, p, s.\n",
        "- A14: continuous.\n",
        "- A15: continuous.\n",
        "- A16: +,- (class attribute)\n",
        "\n",
        "Yes, most of that doesn't mean anything. A16 (the class attribute) is the most interesting, as it separates the 307 approved cases from the 383 rejected cases. The remaining variables have been obfuscated for privacy - a challenge you may have to deal with in your data science career.\n",
        "\n",
        "Sprint challenges are evaluated based on satisfactory completion of each part. It is suggested you work through it in order, getting each aspect reasonably working, before trying to deeply explore, iterate, or refine any given step. Once you get to the end, if you want to go back and improve things, go for it!"
      ]
    },
    {
      "metadata": {
        "id": "5wch6ksCbJtZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Load and validate the data\n",
        "\n",
        "- Load the data as a `pandas` data frame.\n",
        "- Validate that it has the appropriate number of observations (you can check the raw file, and also read the dataset description from UCI).\n",
        "- UCI says there should be missing data - check, and if necessary change the data so pandas recognizes it as na\n",
        "- Make sure that the loaded features are of the types described above (continuous values should be treated as float), and correct as necessary\n",
        "\n",
        "This is review, but skills that you'll use at the start of any data exploration. Further, you may have to do some investigation to figure out which file to load from - that is part of the puzzle."
      ]
    },
    {
      "metadata": {
        "id": "b0TcmHK4pMj9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4020202b-2621-46b3-f1ab-2c68cc170ae1"
      },
      "cell_type": "code",
      "source": [
        "!wc -l crx.data"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "690 crx.data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q79xDLckzibS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "a8c442ef-feab-43ee-9228-63c4221bc75d"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\", header=None)\n",
        "df.columns = [\"A{}\".format(i) for i in range(1, 17)]\n",
        "\n",
        "print(df.count())\n",
        "print()\n",
        "\n",
        "for a in [\"A9\", \"A10\", \"A12\"]:\n",
        "  df[a] = df[a].map({\"t\": True, \"f\": False})\n",
        "df[\"A16\"] = df[\"A16\"].map({\"+\": True, \"-\": False})\n",
        "df[\"A1\"] = df[\"A1\"].apply(lambda x: \"?\" if x not in [\"a\", \"b\"] else x)\n",
        "df[\"A2\"] = df[\"A2\"].apply(lambda x: None if x == \"?\" else float(x))\n",
        "df[\"A4\"] = df[\"A4\"].apply(lambda x: \"?\" if x not in [\"u\", \"y\", \"l\", \"t\"] else x)\n",
        "df[\"A5\"] = df[\"A5\"].apply(lambda x: \"?\" if x not in [\"g\", \"p\", \"gg\"] else x)\n",
        "df[\"A6\"] = df[\"A6\"].apply(lambda x: \"?\" if x not in [\"c\", \"d\", \"cc\", \"i\", \"j\", \"k\", \"m\", \"r\", \"q\", \"w\", \"x\", \"e\", \"aa\", \"ff\"] else x)\n",
        "df[\"A7\"] = df[\"A7\"].apply(lambda x: \"?\" if x not in [\"v\", \"h\", \"bb\", \"j\", \"n\", \"z\", \"dd\", \"ff\", \"o\"] else x)\n",
        "df[\"A14\"] = df[\"A14\"].apply(lambda x: None if x == \"?\" else float(x))\n",
        "\n",
        "for a in df:\n",
        "  print(a, df[a].dtype)\n",
        "print()\n",
        "print(\"NA Values\")\n",
        "df.isna().sum()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A1     690\n",
            "A2     690\n",
            "A3     690\n",
            "A4     690\n",
            "A5     690\n",
            "A6     690\n",
            "A7     690\n",
            "A8     690\n",
            "A9     690\n",
            "A10    690\n",
            "A11    690\n",
            "A12    690\n",
            "A13    690\n",
            "A14    690\n",
            "A15    690\n",
            "A16    690\n",
            "dtype: int64\n",
            "\n",
            "A1 object\n",
            "A2 float64\n",
            "A3 float64\n",
            "A4 object\n",
            "A5 object\n",
            "A6 object\n",
            "A7 object\n",
            "A8 float64\n",
            "A9 bool\n",
            "A10 bool\n",
            "A11 int64\n",
            "A12 bool\n",
            "A13 object\n",
            "A14 float64\n",
            "A15 int64\n",
            "A16 bool\n",
            "\n",
            "NA Values\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "A1      0\n",
              "A2     12\n",
              "A3      0\n",
              "A4      0\n",
              "A5      0\n",
              "A6      0\n",
              "A7      0\n",
              "A8      0\n",
              "A9      0\n",
              "A10     0\n",
              "A11     0\n",
              "A12     0\n",
              "A13     0\n",
              "A14    13\n",
              "A15     0\n",
              "A16     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "metadata": {
        "id": "G7rLytbrO38L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Exploring data, Testing hypotheses\n",
        "\n",
        "The only thing we really know about this data is that A16 is the class label. Besides that, we have 6 continuous (float) features and 9 categorical features.\n",
        "\n",
        "Explore the data: you can use whatever approach (tables, utility functions, visualizations) to get an impression of the distributions and relationships of the variables. In general, your goal is to understand how the features are different when grouped by the two class labels (`+` and `-`).\n",
        "\n",
        "For the 6 continuous features, how are they different when split between the two class labels? Choose two features to run t-tests (again split by class label) - specifically, select one feature that is *extremely* different between the classes, and another feature that is notably less different (though perhaps still \"statistically significantly\" different). You may have to explore more than two features to do this.\n",
        "\n",
        "For the categorical features, explore by creating \"cross tabs\" between them and the class label, and apply the Chi-squared test to them. There are 9 categorical features - as with the t-test, try to find one where the Chi-squared test returns an extreme result (rejecting the null that the data are independent), and one where it is less extreme.\n",
        "\n",
        "**NOTE** - \"less extreme\" just means smaller test statistic/larger p-value. Even the least extreme differences may be strongly statistically significant.\n",
        "\n",
        "Your *main* goal is the hypothesis tests, so don't spend too much time on the exploration/visualization piece. That is just a means to an end. This is challenging, so manage your time and aim for a baseline of at least running two t-tests and two Chi-squared tests before polishing. And don't forget to answer the questions in part 3, even if your results in this part aren't what you want them to be."
      ]
    },
    {
      "metadata": {
        "id": "_nqcgc0yzm68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1904
        },
        "outputId": "077715a6-abce-4340-ae17-0a9b32cba167"
      },
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "\n",
        "print(\"Column\\t\\t- Mean\\t- SEM\\t\\t+ Mean\\t+ SEM\\t\\tP (ttest)\\tDifferent\")\n",
        "a16_df = list(df.groupby(\"A16\"))\n",
        "for a in [2, 3, 8, 11, 14, 15]:\n",
        "  vals = []\n",
        "  for a16 in a16_df:\n",
        "    vals.append(list(a16[1][\"A{}\".format(a)].dropna().values))\n",
        "  pval = stats.ttest_ind(vals[0], vals[1], equal_var=False).pvalue\n",
        "  v0 = stats.describe(vals[0])\n",
        "  v1 = stats.describe(vals[1])\n",
        "  print(\"A{}\\t\\t{:.2f}\\t{:.2f}\\t\\t{:.2f}\\t{:.2f}\\t\\t{:.4f}\\t\\t{}\".format(a, v0.mean, stats.sem(vals[0]), v1.mean, stats.sem(vals[1]), pval, pval < 0.05))\n",
        "  \n",
        "print()  \n",
        "print()\n",
        "\n",
        "print(\"Column\\t\\tP (ChiSquare)\")\n",
        "for a in [1,4,5,6,7,9,10,12,13]:\n",
        "  vals = []\n",
        "  nam = \"\"\n",
        "  ratio = df[df[\"A16\"] == True][\"A16\"].count() / df[\"A16\"].count()\n",
        "  for v in sorted(set(df[\"A{}\".format(a)])):\n",
        "      nam += \"\\t{}\".format(v)\n",
        "  for a16 in a16_df:\n",
        "    val = []\n",
        "    for v in sorted(set(df[\"A{}\".format(a)])):\n",
        "      val.append(a16[1][a16[1][\"A{}\".format(a)] == v][\"A{}\".format(a)].count())\n",
        "    vals.append(val)\n",
        "  print()\n",
        "  print(\"A{}\\t\".format(a) + nam)\n",
        "  chi = stats.chisquare(vals, f_exp=[[ratio * df[df[\"A{}\".format(a)] == v][\"A{}\".format(a)].count() for v in sorted(set(df[\"A{}\".format(a)]))], [(1 - ratio) * df[df[\"A{}\".format(a)] == v][\"A{}\".format(a)].count() for v in sorted(set(df[\"A{}\".format(a)]))]])\n",
        "  s = \"\"\n",
        "  for i in chi[1]:\n",
        "    s += \"\\t{:.4f}\".format(i)\n",
        "  print(\"\\t\" + s)\n",
        "  \n",
        "print()  \n",
        "print()\n",
        "\n",
        "for a in [1,4,5,6,7,9,10,12,13]:\n",
        "  print(pd.crosstab(df[\"A16\"], df[\"A{}\".format(a)], dropna=False, margins=True))\n",
        "  print()\n",
        "  print()\n",
        "  print()"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Column\t\t- Mean\t- SEM\t\t+ Mean\t+ SEM\t\tP (ttest)\tDifferent\n",
            "A2\t\t29.81\t0.57\t\t33.72\t0.73\t\t0.0000\t\tTrue\n",
            "A3\t\t3.84\t0.22\t\t5.90\t0.31\t\t0.0000\t\tTrue\n",
            "A8\t\t1.26\t0.11\t\t3.43\t0.24\t\t0.0000\t\tTrue\n",
            "A11\t\t0.63\t0.10\t\t4.61\t0.36\t\t0.0000\t\tTrue\n",
            "A14\t\t199.70\t9.36\t\t164.42\t9.32\t\t0.0078\t\tTrue\n",
            "A15\t\t198.61\t34.32\t\t2038.86\t437.17\t\t0.0000\t\tTrue\n",
            "\n",
            "\n",
            "Column\t\tP (ChiSquare)\n",
            "\n",
            "A1\t\t?\ta\tb\n",
            "\t\t0.0335\t0.0099\t0.0000\n",
            "\n",
            "A4\t\t?\tl\tu\ty\n",
            "\t\t0.5823\t0.2055\t0.0046\t0.0000\n",
            "\n",
            "A5\t\t?\tg\tgg\tp\n",
            "\t\t0.5823\t0.0046\t0.2055\t0.0000\n",
            "\n",
            "A6\t\t?\taa\tc\tcc\td\te\tff\ti\tj\tk\tm\tq\tr\tw\tx\n",
            "\t\t0.5042\t0.0027\t0.0158\t0.0498\t0.0004\t0.9605\t0.0000\t0.0000\t0.1046\t0.0001\t0.0964\t0.0792\t0.6973\t0.5254\t0.0004\n",
            "\n",
            "A7\t\t?\tbb\tdd\tff\th\tj\tn\to\tv\tz\n",
            "\t\t0.5042\t0.0423\t0.2744\t0.0000\t0.0748\t0.3054\t0.8246\t0.8755\t0.0000\t0.2672\n",
            "\n",
            "A9\t\tFalse\tTrue\n",
            "\t\t0.0000\t0.0000\n",
            "\n",
            "A10\t\tFalse\tTrue\n",
            "\t\t0.0000\t0.0000\n",
            "\n",
            "A12\t\tFalse\tTrue\n",
            "\t\t0.0000\t0.0009\n",
            "\n",
            "A13\t\tg\tp\ts\n",
            "\t\t0.0000\t0.6906\t0.0000\n",
            "\n",
            "\n",
            "A1      ?    a    b  All\n",
            "A16                     \n",
            "False   9  112  262  383\n",
            "True    3   98  206  307\n",
            "All    12  210  468  690\n",
            "\n",
            "\n",
            "\n",
            "A4     ?  l    u    y  All\n",
            "A16                       \n",
            "False  2  0  263  118  383\n",
            "True   4  2  256   45  307\n",
            "All    6  2  519  163  690\n",
            "\n",
            "\n",
            "\n",
            "A5     ?    g  gg    p  All\n",
            "A16                        \n",
            "False  2  263   0  118  383\n",
            "True   4  256   2   45  307\n",
            "All    6  519   2  163  690\n",
            "\n",
            "\n",
            "\n",
            "A6     ?  aa    c  cc   d   e  ff   i   j   k   m   q  r   w   x  All\n",
            "A16                                                                  \n",
            "False  5  35   75  12  23  11  46  45   7  37  22  27  1  31   6  383\n",
            "True   4  19   62  29   7  14   7  14   3  14  16  51  2  33  32  307\n",
            "All    9  54  137  41  30  25  53  59  10  51  38  78  3  64  38  690\n",
            "\n",
            "\n",
            "\n",
            "A7     ?  bb  dd  ff    h  j  n  o    v  z  All\n",
            "A16                                            \n",
            "False  5  34   4  49   51  5  2  1  230  2  383\n",
            "True   4  25   2   8   87  3  2  1  169  6  307\n",
            "All    9  59   6  57  138  8  4  2  399  8  690\n",
            "\n",
            "\n",
            "\n",
            "A9     False  True  All\n",
            "A16                    \n",
            "False    306    77  383\n",
            "True      23   284  307\n",
            "All      329   361  690\n",
            "\n",
            "\n",
            "\n",
            "A10    False  True  All\n",
            "A16                    \n",
            "False    297    86  383\n",
            "True      98   209  307\n",
            "All      395   295  690\n",
            "\n",
            "\n",
            "\n",
            "A12    False  True  All\n",
            "A16                    \n",
            "False    213   170  383\n",
            "True     161   146  307\n",
            "All      374   316  690\n",
            "\n",
            "\n",
            "\n",
            "A13      g  p   s  All\n",
            "A16                   \n",
            "False  338  3  42  383\n",
            "True   287  5  15  307\n",
            "All    625  8  57  690\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZM8JckA2bgnp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Analysis and Interpretation\n",
        "\n",
        "Now that you've looked at the data, answer the following questions:\n",
        "\n",
        "- Interpret and explain the two t-tests you ran - what do they tell you about the relationships between the continuous features you selected and the class labels?\n",
        "- Interpret and explain the two Chi-squared tests you ran - what do they tell you about the relationships between the categorical features you selected and the class labels?\n",
        "- What was the most challenging part of this sprint challenge?\n",
        "\n",
        "Answer with text, but feel free to intersperse example code/results or refer to it from earlier."
      ]
    },
    {
      "metadata": {
        "id": "LIozLDNG2Uhu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For all continuous variables, the p-values using Welch's t test are all less than 0.01, meaning it is highly likely that the distributions of each class is independent when looking at individual continuous variables. This shows that for all continuous variables, there is not much overlap between the distributions for the two class variables, meaning that they could be good predictors for class attribute.\n",
        "\n",
        "For the categorical variables, the comparisons need to be done on a case by case basis. However, what the chi-squared p values show is how each category in the variable is representative of the whole distribution of the class attributes. The majority of the p-values fall under 0.01, meaning rejection of the null hypothesis that the category is representative of the categorical variable distribution. The following categories for each categorical variable are those where the null hypothesis is accepted:\n",
        "\n",
        "- A1: ?\n",
        "- A4: ?, l\n",
        "- A5: ?, gg\n",
        "- A6: ?, c, cc, e, j, m, q, r, w\n",
        "- A7: ?, bb, dd, h, j, n, o, z\n",
        "- A13: p\n",
        "\n",
        "As shown above in the printed crosstab table, the probable reason due to the higher p-values could be that theren just isn't enough data for each category. However for A6(c,cc,e,m,q,w), and A7(bb,h), these categories have counts of over 20, and we can therefore determine that these values are not representative of the class attribute ratio.\n",
        "\n",
        "The most challenging part of the sprint challenge was trying to understand more about the scipy stats chisquare function and using it properly."
      ]
    },
    {
      "metadata": {
        "id": "hNL7_jXGxNhV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}