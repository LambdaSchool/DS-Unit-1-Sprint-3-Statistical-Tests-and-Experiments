{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS_Unit_1_Sprint_Challenge_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zarrinan/DS-Unit-1-Sprint-4-Statistical-Tests-and-Experiments/blob/master/DS_Unit_1_Sprint_Challenge_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NooAiTdnafkz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Science Unit 1 Sprint Challenge 4\n",
        "\n",
        "## Exploring Data, Testing Hypotheses\n",
        "\n",
        "In this sprint challenge you will look at a dataset of people being approved or rejected for credit.\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Credit+Approval\n",
        "\n",
        "Data Set Information: This file concerns credit card applications. All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data. This dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values. There are also a few missing values.\n",
        "\n",
        "Attribute Information:\n",
        "- A1: b, a.\n",
        "- A2: continuous.\n",
        "- A3: continuous.\n",
        "- A4: u, y, l, t.\n",
        "- A5: g, p, gg.\n",
        "- A6: c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff.\n",
        "- A7: v, h, bb, j, n, z, dd, ff, o.\n",
        "- A8: continuous.\n",
        "- A9: t, f.\n",
        "- A10: t, f.\n",
        "- A11: continuous.\n",
        "- A12: t, f.\n",
        "- A13: g, p, s.\n",
        "- A14: continuous.\n",
        "- A15: continuous.\n",
        "- A16: +,- (class attribute)\n",
        "\n",
        "Yes, most of that doesn't mean anything. A16 (the class attribute) is the most interesting, as it separates the 307 approved cases from the 383 rejected cases. The remaining variables have been obfuscated for privacy - a challenge you may have to deal with in your data science career.\n",
        "\n",
        "Sprint challenges are evaluated based on satisfactory completion of each part. It is suggested you work through it in order, getting each aspect reasonably working, before trying to deeply explore, iterate, or refine any given step. Once you get to the end, if you want to go back and improve things, go for it!"
      ]
    },
    {
      "metadata": {
        "id": "5wch6ksCbJtZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Load and validate the data\n",
        "\n",
        "- Load the data as a `pandas` data frame.\n",
        "- Validate that it has the appropriate number of observations (you can check the raw file, and also read the dataset description from UCI).\n",
        "- UCI says there should be missing data - check, and if necessary change the data so pandas recognizes it as na\n",
        "- Make sure that the loaded features are of the types described above (continuous values should be treated as float), and correct as necessary\n",
        "\n",
        "This is review, but skills that you'll use at the start of any data exploration. Further, you may have to do some investigation to figure out which file to load from - that is part of the puzzle."
      ]
    },
    {
      "metadata": {
        "id": "Q79xDLckzibS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import chisquare"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6BncfaOHabQU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "ddebe7b0-4605-4788-9a1d-8dc4a6d3468a"
      },
      "cell_type": "code",
      "source": [
        "# Load the dataset, rename the columns, and see the head, description, A16 column value counts\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data'\n",
        "col_names = ['A1','A2','A3','A4','A5','A6','A7','A8','A9','A10','A11','A12','A13','A14','A15','A16']\n",
        "df_raw = pd.read_csv(url, header=None, names=col_names)\n",
        "print(df_raw.head())\n",
        "print(df_raw.describe())\n",
        "df_raw.A16.value_counts()\n",
        "\n",
        "#I see from the data description file, the Number of Instances: 690, which is \n",
        "#confirmed by the count in dataframe description below. Also, the class distribution\n",
        "#in dataset description is confirmed by the result of A16 column value_counts() result"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  A1     A2     A3 A4 A5 A6 A7    A8 A9 A10  A11 A12 A13    A14  A15 A16\n",
            "0  b  30.83  0.000  u  g  w  v  1.25  t   t    1   f   g  00202    0   +\n",
            "1  a  58.67  4.460  u  g  q  h  3.04  t   t    6   f   g  00043  560   +\n",
            "2  a  24.50  0.500  u  g  q  h  1.50  t   f    0   f   g  00280  824   +\n",
            "3  b  27.83  1.540  u  g  w  v  3.75  t   t    5   t   g  00100    3   +\n",
            "4  b  20.17  5.625  u  g  w  v  1.71  t   f    0   f   s  00120    0   +\n",
            "               A3          A8        A11            A15\n",
            "count  690.000000  690.000000  690.00000     690.000000\n",
            "mean     4.758725    2.223406    2.40000    1017.385507\n",
            "std      4.978163    3.346513    4.86294    5210.102598\n",
            "min      0.000000    0.000000    0.00000       0.000000\n",
            "25%      1.000000    0.165000    0.00000       0.000000\n",
            "50%      2.750000    1.000000    0.00000       5.000000\n",
            "75%      7.207500    2.625000    3.00000     395.500000\n",
            "max     28.000000   28.500000   67.00000  100000.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-    383\n",
              "+    307\n",
              "Name: A16, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "hUW43mvWdQa-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#check if there are missing values, UCI says there are missing values:\n",
        "#     A1:  12\n",
        "#     A2:  12\n",
        "#     A4:   6\n",
        "#     A5:   6\n",
        "#     A6:   9\n",
        "#     A7:   9\n",
        "#     A14: 13\n",
        "      \n",
        "df_raw.isnull().sum() # missing values are not entered as NaN\n",
        "print(df_raw.A1.unique())\n",
        "print(df_raw.A6.unique())\n",
        "print(df_raw.A14.unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EKJInxMZaxp1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#replace missing values, denotes as '?' with NaN\n",
        "df_raw.replace('?', np.nan, inplace = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uFm6zU9se4bh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "6463b5a0-53a9-4d16-aba8-f146d53b0360"
      },
      "cell_type": "code",
      "source": [
        "#check for NaN in the dataframe\n",
        "df_raw.isnull().sum()\n",
        "\n",
        "#the data corresponds to the missing data information in the dataset"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "A1     12\n",
              "A2     12\n",
              "A3      0\n",
              "A4      6\n",
              "A5      6\n",
              "A6      9\n",
              "A7      9\n",
              "A8      0\n",
              "A9      0\n",
              "A10     0\n",
              "A11     0\n",
              "A12     0\n",
              "A13     0\n",
              "A14    13\n",
              "A15     0\n",
              "A16     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "chXkEv34ewtM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "b3498c79-fa68-4254-8574-5f358784c49b"
      },
      "cell_type": "code",
      "source": [
        "#before moving to data exploration, I'll drop nan values, since there are 690 instances of data\n",
        "df_clean = df_raw.dropna()\n",
        "df_clean.isnull().sum()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "A1     0\n",
              "A2     0\n",
              "A3     0\n",
              "A4     0\n",
              "A5     0\n",
              "A6     0\n",
              "A7     0\n",
              "A8     0\n",
              "A9     0\n",
              "A10    0\n",
              "A11    0\n",
              "A12    0\n",
              "A13    0\n",
              "A14    0\n",
              "A15    0\n",
              "A16    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "G7rLytbrO38L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Exploring data, Testing hypotheses\n",
        "\n",
        "The only thing we really know about this data is that A16 is the class label. Besides that, we have 6 continuous (float) features and 9 categorical features.\n",
        "\n",
        "Explore the data: you can use whatever approach (tables, utility functions, visualizations) to get an impression of the distributions and relationships of the variables. In general, your goal is to understand how the features are different when grouped by the two class labels (`+` and `-`).\n",
        "\n",
        "For the 6 continuous features, how are they different when split between the two class labels? Choose two features to run t-tests (again split by class label) - specifically, select one feature that is *extremely* different between the classes, and another feature that is notably less different (though perhaps still \"statistically significantly\" different). You may have to explore more than two features to do this.\n",
        "\n",
        "For the categorical features, explore by creating \"cross tabs\" between them and the class label, and apply the Chi-squared test to them. There are 9 categorical features - as with the t-test, try to find one where the Chi-squared test returns an extreme result (rejecting the null that the data are independent), and one where it is less extreme.\n",
        "\n",
        "**NOTE** - \"less extreme\" just means smaller test statistic/larger p-value. Even the least extreme differences may be strongly statistically significant.\n",
        "\n",
        "Your *main* goal is the hypothesis tests, so don't spend too much time on the exploration/visualization piece. That is just a means to an end. This is challenging, so manage your time and aim for a baseline of at least running two t-tests and two Chi-squared tests before polishing. And don't forget to answer the questions in part 3, even if your results in this part aren't what you want them to be."
      ]
    },
    {
      "metadata": {
        "id": "_nqcgc0yzm68",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# - divide dataframe into two by A16 class attribute\n",
        "\n",
        "# - find 6 continious features = put the in a list named 'cont_feat'\n",
        "# - run t_test for at least 3 continuous feature iterating through the 'cont_feat' list \n",
        "#   appliying t_test function, and saving the result in a dict \n",
        "# - select one feature that is extremely different between the classes, \n",
        "#   and another feature that is notably less different\n",
        "\n",
        "# - find categorical features, put the in a list of 'cat_feat'\n",
        "# - iterating through the list do crosstab, and apply chi-square, and find extreme results\n",
        "#   (rejecting the null that the data are independent), and one where it is less extreme "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LCGlMUPDjnXp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#divide dataframe into 2 by approved and rejected credit cards application results\n",
        "df_pos = df_clean[df_clean.A16 == '+'].copy()\n",
        "df_neg = df_clean[df_clean.A16 == '-'].copy()\n",
        "\n",
        "#check if it's done correctly\n",
        "print(df_pos.A16.unique())\n",
        "print(df_neg.A16.unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ljo0AFNrpaOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "bece97b4-f06a-4558-f743-cca72529f45e"
      },
      "cell_type": "code",
      "source": [
        "df_pos.dtypes"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "A1      object\n",
              "A2      object\n",
              "A3     float64\n",
              "A4      object\n",
              "A5      object\n",
              "A6      object\n",
              "A7      object\n",
              "A8     float64\n",
              "A9      object\n",
              "A10     object\n",
              "A11      int64\n",
              "A12     object\n",
              "A13     object\n",
              "A14     object\n",
              "A15      int64\n",
              "A16     object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "f6ZSYE_7kesg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#check the df dtypes \n",
        "df_pos.dtypes\n",
        "\n",
        "#put continuous and categorical features into lists\n",
        "# cont_feat_0 = ['A2','A3','A8','A11','A14','A15']\n",
        "cont_feat = ['A3','A8','A11','A15']\n",
        "\n",
        "cat_feat =  ['A1','A4','A5','A6','A7','A9','A10','A12','A13']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pbjTvO6MnMAB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#t_test for two independent means in t-distribution\n",
        "def t_test_two_ind_means(a, b, alpha=0.025, tails=2):\n",
        "   \n",
        "    #transform data series into arrays and calculate their lengths and variances\n",
        "    a, b = np.array(a), np.array(b)\n",
        "    na, nb = len(a), len(b)\n",
        "    va, vb = a.var(), b.var()\n",
        "    \n",
        "    #degree of freedom, following the conservative way, choosing the minimum from two\n",
        "    df = min(na - 1, nb - 1)\n",
        "    \n",
        "    #the difference btw two samples' means and SE for the two means\n",
        "    mean_diff = a.mean() - b.mean()\n",
        "    stn_error = np.sqrt(va/na + vb/nb)\n",
        "    \n",
        "    #t_stats and p_value\n",
        "    t_stat = mean_diff / stn_error\n",
        "    p_value = tails * stats.t.sf(abs(t_stat), df)\n",
        "    \n",
        "    #confidence levels\n",
        "    ci_low = mean_diff - stats.t.ppf(1-alpha, df) * stn_error\n",
        "    ci_high = mean_diff + stats.t.ppf(1-alpha, df) * stn_error\n",
        "   \n",
        "    return (t_stat, p_value, mean_diff, ci_low, ci_high)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYBLegR6nYlh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "19579b9b-e994-4543-bbcb-d5554844728c"
      },
      "cell_type": "code",
      "source": [
        "# t_test for continuous features in 'cont_feat' list \n",
        "t_tests = {}\n",
        "\n",
        "for i in range (0, len(cont_feat)):\n",
        "  t_tests[cont_feat[i]] = t_test_two_ind_means(df_pos[cont_feat[i]], df_neg[cont_feat[i]])\n",
        "\n",
        "approved = []\n",
        "rejected = []\n",
        "  \n",
        "#print feature name, t-stat, and p-value for each feature\n",
        "\n",
        "#if t_stat > 0, approved t-stat is higher, add to approved list and \n",
        "#if t_stat < 0, rejected t-stat is higher, add to rejected list\n",
        "\n",
        "for feature in t_tests.keys():\n",
        "  print (feature, 'T-Statistic:', t_tests[feature][0], 'p-value:', t_tests[feature][1])\n",
        "  if t_tests[feature][0] > 0:\n",
        "    approved.append(feature)\n",
        "  elif t_tests[feature][0] < 0:  \n",
        "    rejected.append(feature)\n",
        "    \n",
        "#select one feature that is extremely different between the classes, and another feature that is notably less different"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A3 T-Statistic: 5.299324589493291 p-value: 2.2795264590997203e-07\n",
            "A8 T-Statistic: 8.511096587423829 p-value: 8.860679585039269e-16\n",
            "A11 T-Statistic: 10.507033700624996 p-value: 3.8154625022031593e-22\n",
            "A15 T-Statistic: 4.0865957676287366 p-value: 5.6476391645654614e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GyLV-YqcqMgo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c69e5f39-18e4-427f-b61a-f0774d079aa2"
      },
      "cell_type": "code",
      "source": [
        "print(approved)\n",
        "print(rejected)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A3', 'A8', 'A11', 'A15']\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ygb5QerVqdC1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Visualize the resulting data in a box plot\n",
        "def plot_data(dictionary):\n",
        "  labels, data = [*zip(*dictionary.items())]  # 'transpose' dict items to parallel key, value lists\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.boxplot(data, whis=0, meanline=True, )\n",
        "  plt.xticks(range(1, len(labels) + 1), labels)\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3uPCof2gqny6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_data(t_tests)\n",
        "# from the plot it is seen that A15 issue had a significant effect on approval decision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DLGxo-Y_re53",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create crosstabs for categorical features, run chisquare test, plot histograms\n",
        "#for each of them\n",
        "\n",
        "#The null hypothesis is that the rows/cols are independent -> low chi square\n",
        "#\"less extreme\" just means smaller test statistic/larger p-value\n",
        "\n",
        "cat_feat =  ['A1','A4','A5','A6','A7','A9','A10','A12','A13']\n",
        "\n",
        "chisquare_results = {}\n",
        " \n",
        "def create_plot_crosstabs(df):\n",
        "  for feature in cat_feat:\n",
        "    df_cross = pd.crosstab(df.A16, df[feature], normalize=True)\n",
        "    chisquare_results[feature] = chisquare(df_cross, axis=None)\n",
        "    print(df_cross)\n",
        "    print(chisquare(df_cross, axis=None))\n",
        "    df_cross.plot.bar();\n",
        "\n",
        "create_plot_crosstabs(df_clean)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UVda1Z7f3mxK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "534fc621-a63d-4434-aa91-d97e5486b0d3"
      },
      "cell_type": "code",
      "source": [
        "chisquare_results"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A1': Power_divergenceResult(statistic=0.15467544071536954, pvalue=0.9845514506871024),\n",
              " 'A10': Power_divergenceResult(statistic=0.23302275514822626, pvalue=0.9720900555984554),\n",
              " 'A12': Power_divergenceResult(statistic=0.017567640457870255, pvalue=0.9993839708701547),\n",
              " 'A13': Power_divergenceResult(statistic=1.5495944034952358, pvalue=0.9072766507666103),\n",
              " 'A4': Power_divergenceResult(statistic=0.9415021727965406, pvalue=0.9671446983396204),\n",
              " 'A5': Power_divergenceResult(statistic=0.9415021727965406, pvalue=0.9671446983396204),\n",
              " 'A6': Power_divergenceResult(statistic=0.577330684858903, pvalue=1.0),\n",
              " 'A7': Power_divergenceResult(statistic=2.7089601767317295, pvalue=0.9999668152444824),\n",
              " 'A9': Power_divergenceResult(statistic=0.5426034628725004, pvalue=0.9094367325568282)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "metadata": {
        "id": "ZM8JckA2bgnp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Analysis and Interpretation\n",
        "\n",
        "Now that you've looked at the data, answer the following questions:\n",
        "\n",
        "- Interpret and explain the two t-tests you ran - what do they tell you about the relationships between the continuous features you selected and the class labels?\n",
        "- Interpret and explain the two Chi-squared tests you ran - what do they tell you about the relationships between the categorical features you selected and the class labels?\n",
        "- What was the most challenging part of this sprint challenge?\n",
        "\n",
        "Answer with text, but feel free to intersperse example code/results or refer to it from earlier."
      ]
    },
    {
      "metadata": {
        "id": "LIozLDNG2Uhu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- I ran **t-test** to compare two independent means between approved and rejected credit cards applications data for continuous features, with significance level = 0.025. The positive T-statistic indicates the mean value of the variable is larger for approved applications then than for rejected.The difference between T-Statistic-values is the **most extreme** for **A11** feature with Tstatistics-value = 10.50 and extremely small p-value: 3.8154625022031593e-22. The feature that is **notably less different** is **A15**, with T-Statistic-value =  4.08, and p-value: 5.6476391645654614e-05, although it's still to low to reject the null hypothesis with significance level 0.025.\n",
        "\n",
        "\n",
        "\n",
        "- The **chisquare test** yields high results for all categorical features with p-value > 0.05, which means  that the credit cards approval/rejection is highly dependent on all features, though the **most extreme** results are for  **A6** feature with p-value=1.0 and **A7** feature with p-value = 0.99. The **less extreme** result was for **A12** feature with statistic = 0.0175 and p-value = 0.9072.\n",
        "\n",
        "- the most challenging for this sprint challenge was the interpretation of the t-test results and visualization, since we're limited in time. During the week, I tried to put the code for the homework assigments into reusable functions, which I used today and it helped me and saved my time a ton.\n"
      ]
    }
  ]
}