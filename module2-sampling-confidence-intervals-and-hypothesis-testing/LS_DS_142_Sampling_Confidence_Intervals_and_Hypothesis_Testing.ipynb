{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS DS 142 Sampling Confidence Intervals and Hypothesis Testing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DimaKav/DS-Unit-1-Sprint-4-Statistical-Tests-and-Experiments/blob/master/module2-sampling-confidence-intervals-and-hypothesis-testing/LS_DS_142_Sampling_Confidence_Intervals_and_Hypothesis_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "838Dmw1kM2LK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lambda School Data Science Module 142\n",
        "## Sampling, Confidence Intervals, and Hypothesis Testing"
      ]
    },
    {
      "metadata": {
        "id": "dbcPKIo5M6Ny",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare - examine other available hypothesis tests\n",
        "\n",
        "If you had to pick a single hypothesis test in your toolbox, t-test would probably be the best choice - but the good news is you don't have to pick just one! Here's some of the others to be aware of:"
      ]
    },
    {
      "metadata": {
        "id": "tlBel8j9M6tB",
        "colab_type": "code",
        "outputId": "811623c9-885a-42e3-c3f7-159ced2ce330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import chisquare  # One-way chi square test\n",
        "\n",
        "# Chi square can take any crosstab/table and test the independence of rows/cols\n",
        "# The null hypothesis is that the rows/cols are independent -> low chi square\n",
        "# The alternative is that there is a dependence -> high chi square\n",
        "# Be aware! Chi square does *not* tell you direction/causation\n",
        "\n",
        "ind_obs = np.array([[1, 1], [2, 2]]).T\n",
        "print(ind_obs)\n",
        "print(chisquare(ind_obs, axis=None))\n",
        "\n",
        "dep_obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n",
        "print(dep_obs)\n",
        "print(chisquare(dep_obs, axis=None))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [1 2]]\n",
            "Power_divergenceResult(statistic=0.6666666666666666, pvalue=0.8810148425137847)\n",
            "[[16 32]\n",
            " [18 24]\n",
            " [16 16]\n",
            " [14 28]\n",
            " [12 20]\n",
            " [12 24]]\n",
            "Power_divergenceResult(statistic=23.31034482758621, pvalue=0.015975692534127565)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nN0BdNiDPxbk",
        "colab_type": "code",
        "outputId": "36426de9-d1b9-4790-ae20-9d5eb578a77a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Distribution tests:\n",
        "# We often assume that something is normal, but it can be important to *check*\n",
        "\n",
        "# For example, later on with predictive modeling, a typical assumption is that\n",
        "# residuals (prediction errors) are normal - checking is a good diagnostic\n",
        "\n",
        "from scipy.stats import normaltest\n",
        "# Poisson models arrival times and is related to the binomial (coinflip)\n",
        "sample = np.random.poisson(5, 1000)\n",
        "print(normaltest(sample))  # Pretty clearly not normal"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NormaltestResult(statistic=38.69323106073592, pvalue=3.961609200867749e-09)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P5t0WhkDReFO",
        "colab_type": "code",
        "outputId": "7d6438bf-8042-4297-a8f7-cef083d22444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "# Kruskal-Wallis H-test - compare the median rank between 2+ groups\n",
        "# Can be applied to ranking decisions/outcomes/recommendations\n",
        "# The underlying math comes from chi-square distribution, and is best for n>5\n",
        "from scipy.stats import kruskal\n",
        "\n",
        "x1 = [1, 3, 5, 7, 9]\n",
        "y1 = [2, 4, 6, 8, 10]\n",
        "print(kruskal(x1, y1))  # x1 is a little better, but not \"significantly\" so\n",
        "\n",
        "x2 = [1, 1, 1]\n",
        "y2 = [2, 2, 2]\n",
        "z = [2, 2]  # Hey, a third group, and of different size!\n",
        "print(kruskal(x2, y2, z))  # x clearly dominates"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)\n",
            "KruskalResult(statistic=7.0, pvalue=0.0301973834223185)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7pT3IP36Rh0b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And there's many more! `scipy.stats` is fairly comprehensive, though there are even more available if you delve into the extended world of statistics packages. As tests get increasingly obscure and specialized, the importance of knowing them by heart becomes small - but being able to look them up and figure them out when they *are* relevant is still important."
      ]
    },
    {
      "metadata": {
        "id": "L1_KRuHCM7BW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Live Lecture - let's explore some more of scipy.stats"
      ]
    },
    {
      "metadata": {
        "id": "qW6k0dorM7Lz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c6e7958-3eb2-4a93-ffea-57f5f525d755"
      },
      "cell_type": "code",
      "source": [
        "# Playing with distributions\n",
        "\n",
        "from scipy.stats import chi2\n",
        "\n",
        "chi2_5 = chi2(5)\n",
        "chi2_5"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<scipy.stats._distn_infrastructure.rv_frozen at 0x7f50198c5ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "7WbcmcnDS26d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebf1425e-3cfd-4ed2-e09d-34d252855285"
      },
      "cell_type": "code",
      "source": [
        "chi2_5.mean()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "3MjwrQgnS5U8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ddb155d-98c7-4137-f83a-6342e01fc3b2"
      },
      "cell_type": "code",
      "source": [
        "chi2_5.median()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.351460191095526"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "gf1v0MbKS-gl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "363ddb6f-17f7-43be-ed38-3c21cc26d807"
      },
      "cell_type": "code",
      "source": [
        "chi2_500 = chi2(500)\n",
        "print(chi2_500.mean(), chi2_500.median())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500.0 499.3334915888738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m3mXTdkDS-ib",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "42610a26-b764-4d33-fa91-0e2e0490eb54"
      },
      "cell_type": "code",
      "source": [
        "dir(chi2_5.pdf)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__func__',\n",
              " '__ge__',\n",
              " '__get__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__self__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "VlMY59aZS_MS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1cd2130a-075a-4a3a-e637-d7b7b09ae3f9"
      },
      "cell_type": "code",
      "source": [
        "# Test how normal a chisquare(500) is\n",
        "from scipy.stats import normaltest\n",
        "\n",
        "normaltest(chi2_500.rvs(100000))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NormaltestResult(statistic=283.7729586883694, pvalue=2.395989795920015e-62)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "g4zD_51sS_OI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1006
        },
        "outputId": "b92ab12b-fac7-42ac-f9d6-83470ea35cd4"
      },
      "cell_type": "code",
      "source": [
        "help(normaltest)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function normaltest in module scipy.stats.stats:\n",
            "\n",
            "normaltest(a, axis=0, nan_policy='propagate')\n",
            "    Test whether a sample differs from a normal distribution.\n",
            "    \n",
            "    This function tests the null hypothesis that a sample comes\n",
            "    from a normal distribution.  It is based on D'Agostino and\n",
            "    Pearson's [1]_, [2]_ test that combines skew and kurtosis to\n",
            "    produce an omnibus test of normality.\n",
            "    \n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    a : array_like\n",
            "        The array containing the sample to be tested.\n",
            "    axis : int or None, optional\n",
            "        Axis along which to compute test. Default is 0. If None,\n",
            "        compute over the whole array `a`.\n",
            "    nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
            "        Defines how to handle when input contains nan. 'propagate' returns nan,\n",
            "        'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
            "        values. Default is 'propagate'.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    statistic : float or array\n",
            "        ``s^2 + k^2``, where ``s`` is the z-score returned by `skewtest` and\n",
            "        ``k`` is the z-score returned by `kurtosistest`.\n",
            "    pvalue : float or array\n",
            "       A 2-sided chi squared probability for the hypothesis test.\n",
            "    \n",
            "    References\n",
            "    ----------\n",
            "    .. [1] D'Agostino, R. B. (1971), \"An omnibus test of normality for\n",
            "           moderate and large sample size\", Biometrika, 58, 341-348\n",
            "    \n",
            "    .. [2] D'Agostino, R. and Pearson, E. S. (1973), \"Tests for departure from\n",
            "           normality\", Biometrika, 60, 613-622\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> from scipy import stats\n",
            "    >>> pts = 1000\n",
            "    >>> np.random.seed(28041990)\n",
            "    >>> a = np.random.normal(0, 1, size=pts)\n",
            "    >>> b = np.random.normal(2, 1, size=pts)\n",
            "    >>> x = np.concatenate((a, b))\n",
            "    >>> k2, p = stats.normaltest(x)\n",
            "    >>> alpha = 1e-3\n",
            "    >>> print(\"p = {:g}\".format(p))\n",
            "    p = 3.27207e-11\n",
            "    >>> if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
            "    ...     print(\"The null hypothesis can be rejected\")\n",
            "    ... else:\n",
            "    ...     print(\"The null hypothesis cannot be rejected\")\n",
            "    The null hypothesis can be rejected\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oE9K5TwDS-ku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6d3932f9-35c5-4f87-fc9c-0fafcf71c010"
      },
      "cell_type": "code",
      "source": [
        "# One way chi-square test\n",
        "\n",
        "from scipy.stats import chisquare\n",
        "\n",
        "chi_data = [[1, 2], [2, 1]]\n",
        "chisquare(chi_data, axis=None)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Power_divergenceResult(statistic=0.6666666666666666, pvalue=0.8810148425137847)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "n3tvCrKCUKZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "897ab151-27f8-449b-d3ff-00b505782993"
      },
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "help(stats.t.ppf)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method ppf in module scipy.stats._distn_infrastructure:\n",
            "\n",
            "ppf(q, *args, **kwds) method of scipy.stats._continuous_distns.t_gen instance\n",
            "    Percent point function (inverse of `cdf`) at q of the given RV.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    q : array_like\n",
            "        lower tail probability\n",
            "    arg1, arg2, arg3,... : array_like\n",
            "        The shape parameter(s) for the distribution (see docstring of the\n",
            "        instance object for more information)\n",
            "    loc : array_like, optional\n",
            "        location parameter (default=0)\n",
            "    scale : array_like, optional\n",
            "        scale parameter (default=1)\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    x : array_like\n",
            "        quantile corresponding to the lower tail probability q.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wy_jwMLVUhjo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Confidence intervals\n",
        "# Similar to hypothesis testing, but centered at sample mean\n",
        "# Generally better than reporting the 'point estimate' (sample mean)\n",
        "# ...point estimates aren't always perfect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "11OzdxWTM7UR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Assignment - Build a confidence interval\n",
        "\n",
        "A confidence interval refers to a neighborhood around some point estimate, the size of which is determined by the desired p-value. For instance, we might say that 52% of Americans prefer tacos to burritos, with a 95% confidence interval of +/- 5%.\n",
        "\n",
        "52% (0.52) is the point estimate, and +/- 5% (the interval $[0.47, 0.57]$) is the confidence interval. \"95% confidence\" means a p-value $\\leq 1 - 0.95 = 0.05$.\n",
        "\n",
        "In this case, the confidence interval includes $0.5$ - which is the natural null hypothesis (that half of Americans prefer tacos and half burritos, thus there is no clear favorite). So in this case, we could use the confidence interval to report that we've failed to reject the null hypothesis.\n",
        "\n",
        "But providing the full analysis with a confidence interval, including a graphical representation of it, can be a helpful and powerful way to tell your story. Done well, it is also more intuitive to a layperson than simply saying \"fail to reject the null hypothesis\" - it shows that in fact the data does *not* give a single clear result (the point estimate) but a whole range of possibilities.\n",
        "\n",
        "How is a confidence interval built, and how should it be interpreted? It does *not* mean that 95% of the data lies in that interval - instead, the frequentist interpretation is \"if we were to repeat this experiment 100 times, we would expect the average result to lie in this interval ~95 times.\"\n",
        "\n",
        "For a 95% confidence interval and a normal(-ish) distribution, you can simply remember that +/-2 standard deviations contains 95% of the probability mass, and so the 95% confidence interval based on a given sample is centered at the mean (point estimate) and has a range of +/- 2 (or technically 1.96) standard deviations.\n",
        "\n",
        "Different distributions/assumptions (90% confidence, 99% confidence) will require different math, but the overall process and interpretation (with a frequentist approach) will be the same.\n",
        "\n",
        "Your assignment - using the data from the prior module ([congressional voting records](https://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records)):\n",
        "\n",
        "1. Generate and numerically represent a confidence interval\n",
        "2. Graphically (with a plot) represent the confidence interval\n",
        "3. Interpret the confidence interval - what does it tell you about the data and its distribution?\n",
        "\n",
        "Stretch goals:\n",
        "\n",
        "1. Write a summary of your findings, mixing prose and math/code/results. *Note* - yes, this is by definition a political topic. It is challenging but important to keep your writing voice *neutral* and stick to the facts of the data. Data science often involves considering controversial issues, so it's important to be sensitive about them (especially if you want to publish).\n",
        "2. Apply the techniques you learned today to your project data or other data of your choice, and write/discuss your findings here."
      ]
    },
    {
      "metadata": {
        "id": "Ckcr4A4FM7cs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO - your code!"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}